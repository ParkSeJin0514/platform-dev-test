# ============================================================================
# Terraform Destroy (Multi-Cloud: AWS / GCP)
# ============================================================================
# ìˆ˜ë™ ì‹¤í–‰ìœ¼ë¡œ í´ë¼ìš°ë“œì™€ ë ˆì´ì–´ ì„ íƒí•˜ì—¬ Destroy
# - AWS: Karpenter ì •ë¦¬, ALB ì •ë¦¬, Terraform destroy
# - GCP: GKE ì •ë¦¬, Terraform destroy
# ============================================================================

name: Terraform Destroy

on:
  workflow_dispatch:
    inputs:
      cloud:
        description: 'í´ë¼ìš°ë“œ ì„ íƒ'
        required: true
        default: 'aws'
        type: choice
        options:
          - aws
          - gcp
      confirm:
        description: 'ì‚­ì œ í™•ì¸ (destroy ì…ë ¥)'
        required: true
        default: ''
      layer:
        description: 'ì‚­ì œ ë ˆì´ì–´ ì„ íƒ'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - bootstrap
          - compute
          - foundation

env:
  TF_VERSION: '1.9.0'
  TG_VERSION: '0.54.0'
  TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"

  # AWS ì„¤ì •
  AWS_REGION: ap-northeast-2
  AWS_PROJECT_NAME: "petclinic-kr"

  # GCP ì„¤ì •
  GCP_PROJECT_ID: kdt2-final-project-t1
  GCP_REGION: asia-northeast3
  GCP_CLUSTER_NAME: petclinic-dr-gke
  GCP_WORKLOAD_IDENTITY_PROVIDER: projects/605820610222/locations/global/workloadIdentityPools/github-pool/providers/github-provider

  # Slack
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # í™•ì¸ ë‹¨ê³„
  # ============================================================================
  confirm:
    name: 'Confirm Destroy'
    runs-on: ubuntu-latest
    steps:
      - name: Check confirmation
        if: github.event.inputs.confirm != 'destroy'
        run: |
          echo "âŒ ì‚­ì œë¥¼ ì§„í–‰í•˜ë ¤ë©´ 'destroy'ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
          exit 1

      - name: Confirmed
        run: |
          echo "âœ… ì‚­ì œê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤."
          echo "Cloud: ${{ github.event.inputs.cloud }}"
          echo "Layer: ${{ github.event.inputs.layer }}"

  # ============================================================================
  # Slack ì•Œë¦¼ - ìŠ¹ì¸ ìš”ì²­
  # ============================================================================
  notify-approval:
    name: Notify & Wait for Approval
    needs: [confirm]
    runs-on: ubuntu-latest
    steps:
      - name: Send Slack Notification - Approval Request
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "ğŸš¨ Terraform Destroy ìŠ¹ì¸ ìš”ì²­",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Cloud:*\n${{ github.event.inputs.cloud }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Layer:*\n${{ github.event.inputs.layer }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*ì‹¤í–‰ì:*\n${{ github.actor }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*í™•ì¸:*\n${{ github.event.inputs.confirm }}"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "ğŸ”´ *ê²½ê³ : ì¸í”„ë¼ ì‚­ì œê°€ ìš”ì²­ë˜ì—ˆìŠµë‹ˆë‹¤!* ìŠ¹ì¸ í›„ Destroyê°€ ì‹¤í–‰ë©ë‹ˆë‹¤."
                    }
                  ]
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "ìŠ¹ì¸í•˜ëŸ¬ ê°€ê¸°",
                        "emoji": true
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
                      "style": "danger"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  # ============================================================================
  # AWS Pre-Cleanup (ìŠ¹ì¸ í›„, AWS ì„ íƒ ì‹œ)
  # ============================================================================
  aws-pre-cleanup:
    name: 'AWS Pre-Cleanup'
    needs: [notify-approval]
    if: github.event.inputs.cloud == 'aws'
    runs-on: ubuntu-latest
    environment: production  # ğŸ‘ˆ ìŠ¹ì¸ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ëŒ€ê¸°
    timeout-minutes: 15
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Configure kubectl
        id: configure-kubectl
        continue-on-error: true
        run: |
          CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
            aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
            echo "âœ… EKS í´ëŸ¬ìŠ¤í„° ì—°ê²°: $CLUSTER_NAME"
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ EKS í´ëŸ¬ìŠ¤í„° ì—†ìŒ - Pre-cleanup ìŠ¤í‚µ"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Cleanup Karpenter Resources
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 8
        run: |
          echo "ğŸ§¹ Karpenter ì •ë¦¬ ì‹œì‘..."

          # ============================================================
          # í•µì‹¬: ArgoCDê°€ Karpenterë¥¼ ë³µêµ¬í•˜ì§€ ëª»í•˜ë„ë¡ ë¨¼ì € ë¹„í™œì„±í™”!
          # ìˆœì„œ: ArgoCD Sync ë¹„í™œì„±í™” â†’ Controller ì¤‘ì§€ â†’ NodePool ì‚­ì œ â†’ EC2 ì¢…ë£Œ
          # ============================================================

          # 0. ArgoCD Karpenter Application Auto-Sync ë¹„í™œì„±í™” (selfHeal ë°©ì§€)
          echo "  0ï¸âƒ£ ArgoCD Karpenter Application Auto-Sync ë¹„í™œì„±í™”..."
          kubectl patch application karpenter -n argocd --type merge \
            -p '{"spec":{"syncPolicy":null}}' 2>/dev/null || true
          kubectl patch application karpenter-config -n argocd --type merge \
            -p '{"spec":{"syncPolicy":null}}' 2>/dev/null || true

          # 1. Karpenter Controller ì¤‘ì§€ (ê°€ì¥ ë¨¼ì €! ìƒˆ ë…¸ë“œ ìƒì„± ë°©ì§€)
          echo "  1ï¸âƒ£ Karpenter Controller ì¤‘ì§€..."
          kubectl scale deployment karpenter -n kube-system --replicas=0 --timeout=60s 2>/dev/null || true

          # Controllerê°€ ì™„ì „íˆ ì¤‘ì§€ë  ë•Œê¹Œì§€ ëŒ€ê¸°
          echo "    Controller ì¤‘ì§€ ëŒ€ê¸° ì¤‘..."
          for i in {1..6}; do
            RUNNING_PODS=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=karpenter --field-selector=status.phase=Running -o name 2>/dev/null | wc -l)
            if [ "$RUNNING_PODS" -eq 0 ]; then
              echo "    âœ… Controller ì¤‘ì§€ ì™„ë£Œ"
              break
            fi
            echo "    ëŒ€ê¸° ì¤‘... ($i/6)"
            sleep 5
          done

          # 2. NodePool ì‚­ì œ (ìƒˆ ë…¸ë“œ ìƒì„± ë°©ì§€ - Controllerê°€ ì¬ì‹œì‘í•´ë„ NodePool ì—†ìœ¼ë©´ ìƒì„± ë¶ˆê°€)
          echo "  2ï¸âƒ£ NodePool ì‚­ì œ..."
          kubectl get nodepools -o name 2>/dev/null | while read np; do
            kubectl patch $np -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $np --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 3. EC2NodeClass ì‚­ì œ
          echo "  3ï¸âƒ£ EC2NodeClass ì‚­ì œ..."
          kubectl get ec2nodeclasses -o name 2>/dev/null | while read ec; do
            kubectl patch $ec -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $ec --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 4. NodeClaim Finalizer ì œê±° ë° ì‚­ì œ
          echo "  4ï¸âƒ£ NodeClaim ì‚­ì œ..."
          kubectl get nodeclaims -o name 2>/dev/null | while read nc; do
            kubectl patch $nc -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $nc --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 5. EC2 ì¸ìŠ¤í„´ìŠ¤ ê°•ì œ ì¢…ë£Œ (Controller ì¤‘ì§€ í›„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ)
          echo "  5ï¸âƒ£ Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ê°•ì œ ì¢…ë£Œ..."
          # tag-key í•„í„° ì‚¬ìš© (Values=*ëŠ” ì™€ì¼ë“œì¹´ë“œë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ)
          KARPENTER_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # ì´ë¦„ íŒ¨í„´ìœ¼ë¡œë„ ê²€ìƒ‰ (karpenter-node íŒ¨í„´)
          KARPENTER_BY_NAME=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*karpenter*" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # ë‘ ê²°ê³¼ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
          ALL_KARPENTER_INSTANCES=$(echo "$KARPENTER_INSTANCES $KARPENTER_BY_NAME" | tr ' ' '\n' | sort -u | tr '\n' ' ' | xargs)

          if [ -n "$ALL_KARPENTER_INSTANCES" ]; then
            echo "    ì¢…ë£Œí•  ì¸ìŠ¤í„´ìŠ¤: $ALL_KARPENTER_INSTANCES"
            aws ec2 terminate-instances --instance-ids $ALL_KARPENTER_INSTANCES 2>/dev/null || true
          else
            echo "    Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          fi

          # 6. Karpenter ë…¸ë“œ ì‚­ì œ (K8sì—ì„œ ë…¸ë“œ ê°ì²´ ì œê±°)
          echo "  6ï¸âƒ£ Karpenter ë…¸ë“œ ì‚­ì œ..."
          KARPENTER_NODES=$(kubectl get nodes -l karpenter.sh/nodepool -o name 2>/dev/null || true)
          if [ -n "$KARPENTER_NODES" ]; then
            echo "$KARPENTER_NODES" | while read node; do
              echo "    ì‚­ì œ ì¤‘: $node"
              kubectl delete $node --force --grace-period=0 --timeout=60s 2>/dev/null || true
            done
          else
            echo "    Karpenter ë…¸ë“œ ì—†ìŒ"
          fi

          # 7. EC2 ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ í™•ì¸ (ìµœëŒ€ 2ë¶„ ëŒ€ê¸°)
          echo "  7ï¸âƒ£ EC2 ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ í™•ì¸..."
          for i in {1..12}; do
            REMAINING=$(aws ec2 describe-instances \
              --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending,stopping,shutting-down" \
              --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)
            if [ -z "$REMAINING" ]; then
              echo "    âœ… ëª¨ë“  Karpenter ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì™„ë£Œ"
              break
            fi
            echo "    ëŒ€ê¸° ì¤‘... ($i/12) - ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤: $REMAINING"
            sleep 10
          done

          # 8. í˜¹ì‹œ ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤ í•œë²ˆ ë” ì¢…ë£Œ ì‹œë„
          FINAL_CHECK=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)
          if [ -n "$FINAL_CHECK" ]; then
            echo "  âš ï¸ ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤ ì¬ì¢…ë£Œ: $FINAL_CHECK"
            aws ec2 terminate-instances --instance-ids $FINAL_CHECK 2>/dev/null || true
          fi

          echo "âœ… Karpenter ì •ë¦¬ ì™„ë£Œ"

      - name: Cleanup ArgoCD Applications
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "ğŸ§¹ ArgoCD ì •ë¦¬ ì‹œì‘..."

          # Application Finalizer ì œê±° ë° ì‚­ì œ
          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $app -n argocd --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          echo "âœ… ArgoCD ì •ë¦¬ ì™„ë£Œ"

      - name: Cleanup Ingress and LoadBalancer Services
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 3
        run: |
          echo "ğŸ§¹ Ingress ë° LoadBalancer ì •ë¦¬ ì‹œì‘..."

          # ëª¨ë“  Ingress ì‚­ì œ
          kubectl get ingress -A -o json 2>/dev/null | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl patch ingress $name -n $ns -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete ingress $name -n $ns --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # LoadBalancer íƒ€ì… Service ì‚­ì œ
          kubectl get svc -A -o json 2>/dev/null | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl delete svc $name -n $ns --timeout=60s 2>/dev/null || true
          done

          echo "âœ… Ingress ë° LoadBalancer ì •ë¦¬ ì™„ë£Œ"

      - name: Force Delete ALB and Target Groups
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "ğŸ§¹ ALB ë° Target Group ê°•ì œ ì‚­ì œ..."

          # ============================================================
          # petclinic VPCì˜ ëª¨ë“  ALB ì‚­ì œ (VPC ê¸°ë°˜ - ëˆ„ë½ ë°©ì§€)
          # ============================================================

          # petclinic VPC ID ì°¾ê¸°
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          # ============================================================
          # 1ë‹¨ê³„: VPC ë‚´ ëª¨ë“  ALB ì‚­ì œ
          # ============================================================
          echo "  1ï¸âƒ£ VPC ë‚´ ëª¨ë“  ALB ì‚­ì œ..."
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "  VPC ID: $VPC_ID"

            # VPC ë‚´ ëª¨ë“  ALB ì¡°íšŒ
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text 2>/dev/null || true)
          else
            echo "  VPC ì—†ìŒ - íŒ¨í„´ ê¸°ë°˜ ì‚­ì œë¡œ fallback"
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'monitoring')].LoadBalancerArn" \
              --output text 2>/dev/null || true)
          fi

          if [ -z "$ALB_ARNS" ]; then
            echo "  ì‚­ì œí•  ALB ì—†ìŒ"
          else
            for alb_arn in $ALB_ARNS; do
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $alb_arn --query 'LoadBalancers[0].LoadBalancerName' --output text 2>/dev/null || echo "unknown")
              echo "  ì‚­ì œ ì¤‘: $ALB_NAME ($alb_arn)"

              # Listener ë¨¼ì € ì‚­ì œ
              for listener_arn in $(aws elbv2 describe-listeners --load-balancer-arn $alb_arn --query 'Listeners[*].ListenerArn' --output text 2>/dev/null); do
                echo "    Listener ì‚­ì œ: $listener_arn"
                aws elbv2 delete-listener --listener-arn $listener_arn 2>/dev/null || true
              done

              # ALB ì‚­ì œ
              aws elbv2 delete-load-balancer --load-balancer-arn $alb_arn 2>/dev/null || true
            done
          fi

          # ============================================================
          # 2ë‹¨ê³„: ALB ì™„ì „ ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 3ë¶„)
          # ============================================================
          echo "  2ï¸âƒ£ ALB ì™„ì „ ì‚­ì œ ëŒ€ê¸°..."
          for i in {1..36}; do
            # VPC ê¸°ë°˜ ë˜ëŠ” íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ë‚¨ì€ ALB í™•ì¸
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              REMAINING_ALBS=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            else
              REMAINING_ALBS=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'monitoring')].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$REMAINING_ALBS" ]; then
              echo "  âœ… ëª¨ë“  ALB ì‚­ì œ ì™„ë£Œ"
              break
            fi

            echo "    ëŒ€ê¸° ì¤‘... ($i/36)"
            echo "$REMAINING_ALBS" | while read name state; do
              echo "      - $name: $state"
            done
            sleep 5
          done

          # ============================================================
          # 3ë‹¨ê³„: Target Group ì‚­ì œ (ALB ì‚­ì œ ì™„ë£Œ í›„)
          # VPC ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  Target Group ì‚­ì œ
          # ============================================================
          echo "  3ï¸âƒ£ Target Group ì‚­ì œ..."

          # Target Group ì‚­ì œ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
          for retry in {1..3}; do
            # VPC ê¸°ë°˜ Target Group ì¡°íšŒ
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              TG_ARNS=$(aws elbv2 describe-target-groups \
                --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
                --output text 2>/dev/null || true)
            else
              TG_ARNS=$(aws elbv2 describe-target-groups \
                --query "TargetGroups[?contains(TargetGroupName, 'k8s') || contains(TargetGroupName, 'petclinic') || contains(TargetGroupName, 'monitoring')].TargetGroupArn" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$TG_ARNS" ]; then
              echo "  âœ… ëª¨ë“  Target Group ì‚­ì œ ì™„ë£Œ"
              break
            fi

            echo "    Target Group ì‚­ì œ ì‹œë„ ($retry/3)..."
            for tg_arn in $TG_ARNS; do
              TG_NAME=$(aws elbv2 describe-target-groups --target-group-arns $tg_arn --query 'TargetGroups[0].TargetGroupName' --output text 2>/dev/null || echo "unknown")
              echo "      ì‚­ì œ ì¤‘: $TG_NAME"
              aws elbv2 delete-target-group --target-group-arn $tg_arn 2>/dev/null || true
            done

            # ì‚­ì œ ëŒ€ê¸°
            sleep 10
          done

          echo "âœ… ALB ë° Target Group ì •ë¦¬ ì™„ë£Œ"

      - name: Wait for ALB Deletion
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "â³ AWS ë¡œë“œë°¸ëŸ°ì„œ ìµœì¢… ì‚­ì œ í™•ì¸..."

          # petclinic VPC ID ì°¾ê¸°
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          # ALB ìµœì¢… ì‚­ì œ í™•ì¸ (ìµœëŒ€ 3ë¶„ ëŒ€ê¸°)
          for i in {1..18}; do
            # VPC ê¸°ë°˜ ALB í™•ì¸
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              ALB_LIST=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            else
              ALB_LIST=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'monitoring')].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$ALB_LIST" ]; then
              echo "âœ… ëª¨ë“  ALB ì‚­ì œ ì™„ë£Œ"
              break
            fi

            echo "  ëŒ€ê¸° ì¤‘... ($i/18)"
            echo "$ALB_LIST" | while read name state; do
              echo "    - $name: $state"
              # deleting ìƒíƒœê°€ ì•„ë‹Œ ALBê°€ ìˆìœ¼ë©´ ë‹¤ì‹œ ì‚­ì œ ì‹œë„
              if [ "$state" != "deleting" ]; then
                ALB_ARN=$(aws elbv2 describe-load-balancers --names "$name" --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || true)
                if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
                  echo "      ì¬ì‚­ì œ ì‹œë„..."
                  aws elbv2 delete-load-balancer --load-balancer-arn "$ALB_ARN" 2>/dev/null || true
                fi
              fi
            done
            sleep 10
          done

          # Target Group ìµœì¢… í™•ì¸ (VPC ê¸°ë°˜)
          echo "â³ Target Group ìµœì¢… í™•ì¸..."
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            TG_REMAINING=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupName" \
              --output text 2>/dev/null || true)
          else
            TG_REMAINING=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?contains(TargetGroupName, 'k8s') || contains(TargetGroupName, 'petclinic') || contains(TargetGroupName, 'monitoring')].TargetGroupName" \
              --output text 2>/dev/null || true)
          fi

          if [ -n "$TG_REMAINING" ]; then
            echo "  ë‚¨ì€ Target Group ì¬ì‚­ì œ ì‹œë„..."
            for tg_name in $TG_REMAINING; do
              TG_ARN=$(aws elbv2 describe-target-groups --names "$tg_name" --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || true)
              if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
                aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null || true
              fi
            done
          fi

          echo "âœ… ALB ìµœì¢… í™•ì¸ ì™„ë£Œ"

      # ============================================================
      # EKS í´ëŸ¬ìŠ¤í„° ì—†ì–´ë„ ALB ì‚­ì œ í•„ìš” (í´ëŸ¬ìŠ¤í„° ì‚­ì œ í›„ì—ë„ ALB ë‚¨ì„ ìˆ˜ ìˆìŒ)
      # ============================================================
      - name: Force Delete Remaining ALB (No Cluster)
        if: steps.configure-kubectl.outputs.cluster_exists != 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ í´ëŸ¬ìŠ¤í„° ì—†ì´ ë‚¨ì€ ALB ì‚­ì œ..."

          # petclinic VPC ID ì°¾ê¸°
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "  petclinic VPC ì—†ìŒ - ìŠ¤í‚µ"
          else
            echo "  VPC ID: $VPC_ID"

            # VPC ë‚´ ëª¨ë“  ALB ì‚­ì œ
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text 2>/dev/null || true)

            for alb_arn in $ALB_ARNS; do
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $alb_arn --query 'LoadBalancers[0].LoadBalancerName' --output text 2>/dev/null || echo "unknown")
              echo "  ì‚­ì œ ì¤‘: $ALB_NAME"

              # Listener ì‚­ì œ
              for listener_arn in $(aws elbv2 describe-listeners --load-balancer-arn $alb_arn --query 'Listeners[*].ListenerArn' --output text 2>/dev/null); do
                aws elbv2 delete-listener --listener-arn $listener_arn 2>/dev/null || true
              done

              # ALB ì‚­ì œ
              aws elbv2 delete-load-balancer --load-balancer-arn $alb_arn 2>/dev/null || true
            done

            # ALB ì‚­ì œ ëŒ€ê¸°
            echo "  ALB ì‚­ì œ ëŒ€ê¸°..."
            for i in {1..18}; do
              REMAINING=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerName" \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  âœ… ëª¨ë“  ALB ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "    ëŒ€ê¸° ì¤‘... ($i/18) - ë‚¨ì€ ALB: $REMAINING"
              sleep 10
            done

            # Target Group ì‚­ì œ
            echo "  Target Group ì‚­ì œ..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text 2>/dev/null || true)
            for tg_arn in $TG_ARNS; do
              aws elbv2 delete-target-group --target-group-arn $tg_arn 2>/dev/null || true
            done
          fi

          echo "âœ… ë‚¨ì€ ALB ì‚­ì œ ì™„ë£Œ"

      - name: Pre-Cleanup Complete
        run: echo "âœ… AWS Pre-Cleanup ì™„ë£Œ"

  # ============================================================================
  # GCP Pre-Cleanup (ìŠ¹ì¸ í›„, GCP ì„ íƒ ì‹œ)
  # ============================================================================
  gcp-pre-cleanup:
    name: 'GCP Pre-Cleanup'
    needs: [notify-approval]
    if: github.event.inputs.cloud == 'gcp'
    runs-on: ubuntu-latest
    environment: production  # ğŸ‘ˆ ìŠ¹ì¸ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ëŒ€ê¸°
    timeout-minutes: 15
    steps:
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: github-actions@${{ env.GCP_PROJECT_ID }}.iam.gserviceaccount.com

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Configure kubectl
        id: configure-kubectl
        continue-on-error: true
        run: |
          if gcloud container clusters describe ${{ env.GCP_CLUSTER_NAME }} --region ${{ env.GCP_REGION }} 2>/dev/null; then
            gcloud container clusters get-credentials ${{ env.GCP_CLUSTER_NAME }} \
              --region ${{ env.GCP_REGION }} \
              --project ${{ env.GCP_PROJECT_ID }}
            echo "âœ… GKE í´ëŸ¬ìŠ¤í„° ì—°ê²°: ${{ env.GCP_CLUSTER_NAME }}"
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ GKE í´ëŸ¬ìŠ¤í„° ì—†ìŒ - Pre-cleanup ìŠ¤í‚µ"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Cleanup ArgoCD Applications
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "ğŸ§¹ ArgoCD ì •ë¦¬ ì‹œì‘..."

          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $app -n argocd --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          echo "âœ… ArgoCD ì •ë¦¬ ì™„ë£Œ"

      - name: Cleanup Ingress and LoadBalancer Services
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "ğŸ§¹ Ingress ë° LoadBalancer ì •ë¦¬ ì‹œì‘..."

          # ëª¨ë“  Ingress ì‚­ì œ
          kubectl get ingress -A -o json 2>/dev/null | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl patch ingress $name -n $ns -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete ingress $name -n $ns --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # LoadBalancer íƒ€ì… Service ì‚­ì œ
          kubectl get svc -A -o json 2>/dev/null | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl delete svc $name -n $ns --timeout=60s 2>/dev/null || true
          done

          echo "âœ… Ingress ë° LoadBalancer ì •ë¦¬ ì™„ë£Œ"

          # GKE Ingress Controllerê°€ LB ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•  ì‹œê°„ ëŒ€ê¸°
          echo "â³ GKE Ingress Controllerê°€ LB ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘... (60ì´ˆ ëŒ€ê¸°)"
          sleep 60

      - name: Cleanup GKE Ingress Resources (LB Stack â†’ Backend â†’ NEG)
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "ğŸ§¹ GKE Ingress ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬, ìµœëŒ€ 10ë¶„)..."
          echo "   ì‚­ì œ ìˆœì„œ: Forwarding Rules â†’ Target Proxies â†’ URL Maps â†’ Backend Services â†’ Health Checks â†’ NEG â†’ Firewall"
          echo "   ì™„ë£Œ ì‹œ ì¦‰ì‹œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰"

          # ============================================================
          # 1-5ë‹¨ê³„: LB ë¦¬ì†ŒìŠ¤ ì‚­ì œ (ë³‘ë ¬)
          # ============================================================
          echo "  1ï¸âƒ£ Forwarding Rules ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute forwarding-rules list --filter="name~k8s OR name~tf-" --format="value(name)" --global 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute forwarding-rules delete "{}" --global --quiet 2>/dev/null || true

          echo "  2ï¸âƒ£ Target Proxies ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute target-http-proxies list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute target-http-proxies delete "{}" --global --quiet 2>/dev/null || true
          gcloud compute target-https-proxies list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute target-https-proxies delete "{}" --global --quiet 2>/dev/null || true

          echo "  3ï¸âƒ£ URL Maps ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute url-maps list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute url-maps delete "{}" --global --quiet 2>/dev/null || true

          echo "  4ï¸âƒ£ Backend Services ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute backend-services list --filter="name~k8s OR name~tf-" --format="value(name)" --global 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute backend-services delete "{}" --global --quiet 2>/dev/null || true

          echo "  5ï¸âƒ£ Health Checks ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute health-checks list --filter="name~k8s" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute health-checks delete "{}" --global --quiet 2>/dev/null || true

          # ============================================================
          # 6ë‹¨ê³„: NEG ì‚­ì œ (ì™„ë£Œ ê¸°ë°˜ ëŒ€ê¸°, ìµœëŒ€ 5ë¶„)
          # ============================================================
          echo "  6ï¸âƒ£ NEG ì‚­ì œ (ë³‘ë ¬, ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì§„í–‰)..."

          for attempt in {1..30}; do  # 30 x 10s = 5ë¶„ ìµœëŒ€
            # í˜„ì¬ NEG ëª©ë¡ í™•ì¸
            NEG_COUNT=$(gcloud compute network-endpoint-groups list \
              --filter="name~k8s1 OR name~k8s2 OR name~petclinic" \
              --format="value(name)" 2>/dev/null | wc -l)

            if [ "$NEG_COUNT" -eq 0 ]; then
              echo "    âœ… ëª¨ë“  NEG ì‚­ì œ ì™„ë£Œ!"
              break
            fi

            echo "    ë‚¨ì€ NEG: $NEG_COUNTê°œ (ì‹œë„ $attempt/30)"

            # zoneë³„ ë³‘ë ¬ ì‚­ì œ ì‹œë„
            for zone in asia-northeast3-a asia-northeast3-b asia-northeast3-c; do
              (
                gcloud compute network-endpoint-groups list \
                  --filter="zone:$zone AND (name~k8s1 OR name~k8s2 OR name~petclinic)" \
                  --format="value(name)" 2>/dev/null | \
                  xargs -P 10 -I {} gcloud compute network-endpoint-groups delete "{}" --zone="$zone" --quiet 2>/dev/null || true
              ) &
            done
            wait

            sleep 10
          done

          # ============================================================
          # 7ë‹¨ê³„: ë°©í™”ë²½ ì‚­ì œ (ë³‘ë ¬)
          # ============================================================
          echo "  7ï¸âƒ£ ë°©í™”ë²½ ê·œì¹™ ì‚­ì œ (ë³‘ë ¬)..."
          gcloud compute firewall-rules list --filter="name~k8s" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute firewall-rules delete "{}" --quiet 2>/dev/null || true

          echo "âœ… GKE Ingress ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ"

      - name: Delete Cloud SQL Instance (Must be deleted before VPC Peering)
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "ğŸ§¹ Cloud SQL ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ (VPC Peering ì‚­ì œ ì „ í•„ìˆ˜)..."

          # Cloud SQL ì¸ìŠ¤í„´ìŠ¤ ëª©ë¡ ì¡°íšŒ ë° ì‚­ì œ
          for instance in $(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null); do
            echo "  Cloud SQL ì‚­ì œ: $instance"
            # deletion protection í•´ì œ
            gcloud sql instances patch "$instance" --no-deletion-protection --quiet 2>/dev/null || true
            # ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ
            gcloud sql instances delete "$instance" --quiet 2>/dev/null || true
          done

          # Cloud SQL ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 8ë¶„ - Cloud SQL ì‚­ì œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
          echo "  Cloud SQL ì‚­ì œ ëŒ€ê¸°..."
          for i in {1..48}; do
            REMAINING=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
            if [ "$REMAINING" -eq 0 ]; then
              echo "  âœ… Cloud SQL ì‚­ì œ ì™„ë£Œ"
              break
            fi
            echo "    ëŒ€ê¸° ì¤‘... ($i/48) - ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤: $REMAINING"
            sleep 10
          done

          # ìµœì¢… í™•ì¸
          FINAL_CHECK=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
          if [ "$FINAL_CHECK" -gt 0 ]; then
            echo "  âš ï¸ Cloud SQL ì¸ìŠ¤í„´ìŠ¤ê°€ ì•„ì§ ì‚­ì œ ì¤‘ì…ë‹ˆë‹¤. Terraformì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
          fi

          echo "âœ… Cloud SQL ì •ë¦¬ ì™„ë£Œ"

      - name: Final Cleanup - NEG and Firewall (Before VPC deletion)
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ VPC ì‚­ì œ ì „ NEG ë° ë°©í™”ë²½ ê·œì¹™ ìµœì¢… ì •ë¦¬ (ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì§„í–‰)..."

          # ============================================================
          # NEG ìµœì¢… ì •ë¦¬ (ì™„ë£Œ ê¸°ë°˜ ëŒ€ê¸°, ìµœëŒ€ 3ë¶„)
          # ============================================================
          echo "  1ï¸âƒ£ ë‚¨ì€ NEG ìµœì¢… ì‚­ì œ..."

          for attempt in {1..18}; do  # 18 x 10s = 3ë¶„ ìµœëŒ€
            # í˜„ì¬ NEG ëª©ë¡ í™•ì¸
            NEG_COUNT=$(gcloud compute network-endpoint-groups list \
              --filter="name~k8s1 OR name~k8s2 OR name~petclinic" \
              --format="value(name)" 2>/dev/null | wc -l)

            if [ "$NEG_COUNT" -eq 0 ]; then
              echo "    âœ… ëª¨ë“  NEG ì‚­ì œ ì™„ë£Œ!"
              break
            fi

            echo "    ë‚¨ì€ NEG: $NEG_COUNTê°œ (ì‹œë„ $attempt/18)"

            # zoneë³„ ë³‘ë ¬ ì‚­ì œ ì‹œë„
            for zone in asia-northeast3-a asia-northeast3-b asia-northeast3-c; do
              (
                gcloud compute network-endpoint-groups list \
                  --filter="zone:$zone AND (name~k8s1 OR name~k8s2 OR name~petclinic)" \
                  --format="value(name)" 2>/dev/null | \
                  xargs -P 10 -I {} gcloud compute network-endpoint-groups delete "{}" --zone="$zone" --quiet 2>/dev/null || true
              ) &
            done
            wait

            sleep 10
          done

          # ìµœì¢… ìƒíƒœ í™•ì¸
          REMAINING=$(gcloud compute network-endpoint-groups list --filter="name~k8s1 OR name~k8s2 OR name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
          if [ "$REMAINING" -gt 0 ]; then
            echo "  âš ï¸ ë‚¨ì€ NEG: $REMAININGê°œ (Terraformì—ì„œ ì²˜ë¦¬)"
          fi

          # ============================================================
          # ë°©í™”ë²½ ê·œì¹™ ìµœì¢… ì •ë¦¬ (ë³‘ë ¬)
          # ============================================================
          echo "  2ï¸âƒ£ ë°©í™”ë²½ ê·œì¹™ ìµœì¢… ì‚­ì œ (ë³‘ë ¬)..."

          # VPC ê´€ë ¨ ë°©í™”ë²½ + k8s íŒ¨í„´ ë°©í™”ë²½ ë³‘ë ¬ ì‚­ì œ
          gcloud compute firewall-rules list --filter="name~k8s OR network~petclinic" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute firewall-rules delete "{}" --quiet 2>/dev/null || true

          echo "âœ… NEG ë° ë°©í™”ë²½ ê·œì¹™ ìµœì¢… ì •ë¦¬ ì™„ë£Œ"

      - name: Cleanup VPC Resources (Peering, Routes, Addresses)
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ VPC ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘..."

          # ============================================================
          # Cloud SQL ì‚­ì œ ì™„ë£Œ í™•ì¸ (Service Networking Connection ì‚­ì œ ì „ í•„ìˆ˜!)
          # ============================================================
          echo "  0ï¸âƒ£ Cloud SQL ì‚­ì œ ì™„ë£Œ í™•ì¸..."
          for i in {1..12}; do
            SQL_COUNT=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
            if [ "$SQL_COUNT" -eq 0 ]; then
              echo "  âœ… Cloud SQL ì‚­ì œ í™•ì¸ë¨"
              break
            fi
            echo "    Cloud SQL ì•„ì§ ì‚­ì œ ì¤‘... ($i/12)"
            sleep 10
          done

          # petclinic ê´€ë ¨ VPC ëª©ë¡ ì¡°íšŒ
          VPCS=$(gcloud compute networks list --filter="name~petclinic" --format="value(name)" 2>/dev/null || true)

          for vpc in $VPCS; do
            echo "  VPC: $vpc"

            # 1. Service Networking Connection ì‚­ì œ (Cloud SQL Private Service Connection)
            echo "    Service Networking Connection ì‚­ì œ..."
            # Cloud SQLì´ ì•„ì§ ì‚­ì œ ì¤‘ì´ë©´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì‹œë„
            for retry in {1..3}; do
              gcloud services vpc-peerings delete \
                --service=servicenetworking.googleapis.com \
                --network=$vpc \
                --quiet 2>/dev/null && break
              echo "    Service Networking ì‚­ì œ ì¬ì‹œë„... ($retry/3)"
              sleep 10
            done

            # 2. VPC Peering ì‚­ì œ
            PEERINGS=$(gcloud compute networks peerings list --network=$vpc --format="value(name)" 2>/dev/null || true)
            for peering in $PEERINGS; do
              echo "    Peering ì‚­ì œ: $peering"
              gcloud compute networks peerings delete $peering --network=$vpc --quiet 2>/dev/null || true
            done

            # 3. VPCì— ì—°ê²°ëœ Route ì‚­ì œ
            ROUTES=$(gcloud compute routes list --filter="network:$vpc" --format="value(name)" 2>/dev/null || true)
            for route in $ROUTES; do
              echo "    Route ì‚­ì œ: $route"
              gcloud compute routes delete $route --quiet 2>/dev/null || true
            done
          done

          # 4. Global Address ì‚­ì œ (Cloud SQL Private IP ë“±)
          echo "  Global Address ì‚­ì œ..."
          for addr in $(gcloud compute addresses list --filter="name~petclinic" --format="value(name)" --global 2>/dev/null); do
            echo "    ì‚­ì œ: $addr"
            gcloud compute addresses delete "$addr" --global --quiet 2>/dev/null || true
          done

          echo "âœ… VPC ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ"

      - name: Pre-Cleanup Complete
        run: echo "âœ… GCP Pre-Cleanup ì™„ë£Œ"

  # ============================================================================
  # Terraform Destroy (ìŠ¹ì¸ ë° Pre-Cleanup í›„ ì‹¤í–‰)
  # ============================================================================
  destroy:
    name: 'Terraform Destroy'
    needs: [notify-approval, aws-pre-cleanup, gcp-pre-cleanup]
    if: always() && needs.notify-approval.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    defaults:
      run:
        working-directory: ${{ github.event.inputs.cloud }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # AWS ì¸ì¦
      - name: Configure AWS credentials
        if: github.event.inputs.cloud == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # GCP ì¸ì¦
      - name: Authenticate to Google Cloud
        if: github.event.inputs.cloud == 'gcp'
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: github-actions@${{ env.GCP_PROJECT_ID }}.iam.gserviceaccount.com

      - name: Set up Cloud SDK
        if: github.event.inputs.cloud == 'gcp'
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Setup Terragrunt
        run: |
          wget -q https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64
          chmod +x terragrunt_linux_amd64
          sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt

      # SSH Key (AWS ì „ìš©)
      - name: Create SSH Key
        if: github.event.inputs.cloud == 'aws'
        run: |
          mkdir -p keys
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > keys/test.pub

      # ArgoCD Finalizer ì œê±° (Namespace ì‚­ì œ íƒ€ì„ì•„ì›ƒ ë°©ì§€)
      - name: Remove ArgoCD Finalizers
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        continue-on-error: true
        run: |
          echo "ğŸ”§ ArgoCD Finalizer ì œê±° ì¤‘..."
          # ArgoCD Application finalizer ì œê±°
          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          # ArgoCD namespace finalizer ì œê±°
          kubectl get namespace argocd -o json 2>/dev/null | \
            jq '.spec.finalizers = []' | \
            kubectl replace --raw "/api/v1/namespaces/argocd/finalize" -f - 2>/dev/null || true
          echo "âœ… Finalizer ì œê±° ì™„ë£Œ"

      # Destroy Bootstrap (ë¨¼ì €! ArgoCD, Helm ë¦´ë¦¬ìŠ¤ ì •ë¦¬)
      - name: Destroy Bootstrap
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        continue-on-error: true
        timeout-minutes: 15
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "ğŸ—‘ï¸ Bootstrap Layer Destroy (ArgoCD, Helm ë¨¼ì € ì‚­ì œ)..."
          cd bootstrap && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      # Destroy Compute (Bootstrap ì‚­ì œ í›„! GKE/EKS, DB ì‚­ì œ)
      - name: Destroy Compute
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'compute'
        continue-on-error: true
        timeout-minutes: 30
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "ğŸ—‘ï¸ Compute Layer Destroy (GKE/EKS, DB ì‚­ì œ)..."
          cd compute && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      # Destroy Foundation
      - name: Destroy Foundation
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'foundation'
        continue-on-error: true
        timeout-minutes: 15
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "ğŸ—‘ï¸ Foundation Layer Destroy..."
          cd foundation && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      - name: Destroy Complete
        run: |
          echo "=============================================="
          echo "ğŸ‰ Terraform Destroy ì™„ë£Œ!"
          echo "=============================================="
          echo "Cloud: ${{ github.event.inputs.cloud }}"
          echo "Layer: ${{ github.event.inputs.layer }}"

  # ============================================================================
  # AWS Post-Cleanup (Terraform Destroy í›„ ë‚¨ì€ ë¦¬ì†ŒìŠ¤ ì •ë¦¬)
  # ============================================================================
  aws-post-cleanup:
    name: 'AWS Post-Cleanup'
    needs: [destroy]
    if: always() && github.event.inputs.cloud == 'aws'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # ============================================================
      # Terraform Destroy ì‹¤íŒ¨ ì‹œ ë‚¨ì€ ë¦¬ì†ŒìŠ¤ ê°•ì œ ì‚­ì œ
      # ============================================================
      - name: Force Delete Karpenter EC2 Instances
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ê°•ì œ ì¢…ë£Œ..."

          # ============================================================
          # Karpenter ë…¸ë“œ ê²€ìƒ‰ (ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰)
          # ============================================================

          # 1. karpenter.sh/nodepool íƒœê·¸ë¡œ ê²€ìƒ‰
          KARPENTER_BY_TAG=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # 2. ì´ë¦„ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰ (karpenter í¬í•¨)
          KARPENTER_BY_NAME=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*karpenter*" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # 3. eks-karpenter-node íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
          KARPENTER_BY_EKS_NAME=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*eks*karpenter*" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # 4. karpenter.sh/discovery íƒœê·¸ë¡œ ê²€ìƒ‰
          KARPENTER_BY_DISCOVERY=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/discovery" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # ëª¨ë“  ê²°ê³¼ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)
          ALL_KARPENTER=$(echo "$KARPENTER_BY_TAG $KARPENTER_BY_NAME $KARPENTER_BY_EKS_NAME $KARPENTER_BY_DISCOVERY" | tr ' ' '\n' | sort -u | grep -v '^$' | tr '\n' ' ' | xargs)

          if [ -n "$ALL_KARPENTER" ]; then
            echo "  ë°œê²¬ëœ Karpenter ì¸ìŠ¤í„´ìŠ¤: $ALL_KARPENTER"
            aws ec2 terminate-instances --instance-ids $ALL_KARPENTER 2>/dev/null || true

            # ì¢…ë£Œ ëŒ€ê¸° (ìµœëŒ€ 2ë¶„)
            echo "  ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°..."
            for i in {1..12}; do
              REMAINING=$(aws ec2 describe-instances \
                --instance-ids $ALL_KARPENTER \
                --query 'Reservations[*].Instances[?State.Name!=`terminated`].InstanceId' \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  âœ… ëª¨ë“  Karpenter ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì™„ë£Œ"
                break
              fi
              echo "    ëŒ€ê¸° ì¤‘... ($i/12) - ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤: $REMAINING"
              sleep 10
            done
          else
            echo "  ì‚­ì œí•  Karpenter ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          fi

      - name: Force Delete Remaining EKS Clusters
        continue-on-error: true
        timeout-minutes: 15
        run: |
          echo "ğŸ§¹ ë‚¨ì€ EKS í´ëŸ¬ìŠ¤í„° ê°•ì œ ì‚­ì œ..."

          # petclinic ê´€ë ¨ EKS í´ëŸ¬ìŠ¤í„° ì¡°íšŒ
          EKS_CLUSTERS=""
          for c in $(aws eks list-clusters --query 'clusters[*]' --output text 2>/dev/null || true); do
            if echo "$c" | grep -qi "petclinic"; then
              EKS_CLUSTERS="$EKS_CLUSTERS $c"
            fi
          done
          EKS_CLUSTERS=$(echo "$EKS_CLUSTERS" | xargs)

          if [ -z "$EKS_CLUSTERS" ]; then
            echo "  ì‚­ì œí•  EKS í´ëŸ¬ìŠ¤í„° ì—†ìŒ"
          else
            for cluster in $EKS_CLUSTERS; do
              echo "  í´ëŸ¬ìŠ¤í„° ì‚­ì œ: $cluster"

              # Node Group ì‚­ì œ
              for ng in $(aws eks list-nodegroups --cluster-name $cluster --query 'nodegroups[*]' --output text 2>/dev/null || true); do
                echo "    Node Group ì‚­ì œ: $ng"
                aws eks delete-nodegroup --cluster-name $cluster --nodegroup-name $ng 2>/dev/null || true
              done

              # Fargate Profile ì‚­ì œ
              for fp in $(aws eks list-fargate-profiles --cluster-name $cluster --query 'fargateProfileNames[*]' --output text 2>/dev/null || true); do
                echo "    Fargate Profile ì‚­ì œ: $fp"
                aws eks delete-fargate-profile --cluster-name $cluster --fargate-profile-name $fp 2>/dev/null || true
              done

              # Add-ons ì‚­ì œ
              for addon in $(aws eks list-addons --cluster-name $cluster --query 'addons[*]' --output text 2>/dev/null || true); do
                aws eks delete-addon --cluster-name $cluster --addon-name $addon 2>/dev/null || true
              done
            done

            # Node Group ì‚­ì œ ëŒ€ê¸°
            echo "  Node Group ì‚­ì œ ëŒ€ê¸°..."
            for i in {1..30}; do
              ALL_DELETED=true
              for cluster in $EKS_CLUSTERS; do
                NG_COUNT=$(aws eks list-nodegroups --cluster-name $cluster --query 'length(nodegroups)' --output text 2>/dev/null || echo "0")
                if [ "$NG_COUNT" != "0" ]; then
                  ALL_DELETED=false
                  break
                fi
              done
              if [ "$ALL_DELETED" = true ]; then
                echo "  âœ… Node Group ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "    ëŒ€ê¸° ì¤‘... ($i/30)"
              sleep 10
            done

            # EKS í´ëŸ¬ìŠ¤í„° ì‚­ì œ
            for cluster in $EKS_CLUSTERS; do
              echo "  EKS í´ëŸ¬ìŠ¤í„° ì‚­ì œ ìš”ì²­: $cluster"
              aws eks delete-cluster --name $cluster 2>/dev/null || true
            done

            # í´ëŸ¬ìŠ¤í„° ì‚­ì œ ëŒ€ê¸°
            echo "  EKS í´ëŸ¬ìŠ¤í„° ì‚­ì œ ëŒ€ê¸°..."
            for i in {1..60}; do
              REMAINING=""
              for c in $(aws eks list-clusters --query 'clusters[*]' --output text 2>/dev/null || true); do
                if echo "$c" | grep -qi "petclinic"; then
                  REMAINING="$REMAINING $c"
                fi
              done
              if [ -z "$(echo $REMAINING | xargs)" ]; then
                echo "  âœ… ëª¨ë“  EKS í´ëŸ¬ìŠ¤í„° ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "    ëŒ€ê¸° ì¤‘... ($i/60) - ë‚¨ì€ í´ëŸ¬ìŠ¤í„°: $REMAINING"
              sleep 10
            done
          fi

      - name: Force Delete Remaining RDS Instances
        continue-on-error: true
        timeout-minutes: 15
        run: |
          echo "ğŸ§¹ ë‚¨ì€ RDS ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ..."

          RDS_INSTANCES=$(aws rds describe-db-instances \
            --query "DBInstances[?contains(DBInstanceIdentifier, 'petclinic')].DBInstanceIdentifier" \
            --output text 2>/dev/null || true)

          if [ -z "$RDS_INSTANCES" ]; then
            echo "  ì‚­ì œí•  RDS ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          else
            for db_id in $RDS_INSTANCES; do
              echo "  RDS ì‚­ì œ: $db_id"
              aws rds modify-db-instance --db-instance-identifier $db_id \
                --no-deletion-protection --apply-immediately 2>/dev/null || true
              sleep 5
              aws rds delete-db-instance --db-instance-identifier $db_id \
                --skip-final-snapshot --delete-automated-backups 2>/dev/null || true
            done

            # RDS ì‚­ì œ ëŒ€ê¸°
            echo "  RDS ì‚­ì œ ëŒ€ê¸°..."
            for i in {1..60}; do
              REMAINING=$(aws rds describe-db-instances \
                --query "DBInstances[?contains(DBInstanceIdentifier, 'petclinic')].DBInstanceIdentifier" \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  âœ… ëª¨ë“  RDS ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "    ëŒ€ê¸° ì¤‘... ($i/60)"
              sleep 10
            done
          fi

          # DB Subnet Group ì‚­ì œ
          echo "  DB Subnet Group ì‚­ì œ..."
          for sg in $(aws rds describe-db-subnet-groups \
            --query "DBSubnetGroups[?contains(DBSubnetGroupName, 'petclinic')].DBSubnetGroupName" \
            --output text 2>/dev/null || true); do
            aws rds delete-db-subnet-group --db-subnet-group-name $sg 2>/dev/null || true
          done

      - name: Force Delete Remaining ALB and Target Groups
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ ë‚¨ì€ ALB ë° Target Group ì‚­ì œ..."

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            # ALB ì‚­ì œ
            for alb_arn in $(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text 2>/dev/null || true); do
              echo "  ALB ì‚­ì œ: $alb_arn"
              aws elbv2 delete-load-balancer --load-balancer-arn $alb_arn 2>/dev/null || true
            done

            # ALB ì‚­ì œ ëŒ€ê¸°
            sleep 30

            # Target Group ì‚­ì œ
            for tg_arn in $(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text 2>/dev/null || true); do
              echo "  Target Group ì‚­ì œ: $tg_arn"
              aws elbv2 delete-target-group --target-group-arn $tg_arn 2>/dev/null || true
            done
          fi

      - name: Force Delete Security Groups
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "ğŸ§¹ ë³´ì•ˆ ê·¸ë£¹ ì‚­ì œ..."

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            SG_LIST=$(aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'SecurityGroups[*].GroupId' --output text 2>/dev/null || true)

            # 1ë‹¨ê³„: ëª¨ë“  ê·œì¹™ ì œê±°
            for sg_id in $SG_LIST; do
              SG_NAME=$(aws ec2 describe-security-groups --group-ids $sg_id \
                --query 'SecurityGroups[0].GroupName' --output text 2>/dev/null || echo "unknown")
              [ "$SG_NAME" = "default" ] && continue

              for rule_id in $(aws ec2 describe-security-group-rules \
                --filters "Name=group-id,Values=$sg_id" \
                --query 'SecurityGroupRules[?IsEgress==`false`].SecurityGroupRuleId' \
                --output text 2>/dev/null || true); do
                [ -z "$rule_id" ] || [ "$rule_id" = "None" ] && continue
                aws ec2 revoke-security-group-ingress --group-id $sg_id \
                  --security-group-rule-ids $rule_id 2>/dev/null || true
              done

              for rule_id in $(aws ec2 describe-security-group-rules \
                --filters "Name=group-id,Values=$sg_id" \
                --query 'SecurityGroupRules[?IsEgress==`true`].SecurityGroupRuleId' \
                --output text 2>/dev/null || true); do
                [ -z "$rule_id" ] || [ "$rule_id" = "None" ] && continue
                aws ec2 revoke-security-group-egress --group-id $sg_id \
                  --security-group-rule-ids $rule_id 2>/dev/null || true
              done
            done

            # 2ë‹¨ê³„: ë³´ì•ˆ ê·¸ë£¹ ì‚­ì œ
            for sg_id in $SG_LIST; do
              SG_NAME=$(aws ec2 describe-security-groups --group-ids $sg_id \
                --query 'SecurityGroups[0].GroupName' --output text 2>/dev/null || echo "unknown")
              [ "$SG_NAME" = "default" ] && continue
              echo "  ì‚­ì œ: $sg_id ($SG_NAME)"
              aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
            done

            # 3ë‹¨ê³„: ì¬ì‹œë„
            for i in {1..3}; do
              REMAINING=$(aws ec2 describe-security-groups \
                --filters "Name=vpc-id,Values=$VPC_ID" \
                --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  âœ… ëª¨ë“  ë³´ì•ˆ ê·¸ë£¹ ì‚­ì œ ì™„ë£Œ"
                break
              fi
              for sg_id in $REMAINING; do
                aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
              done
              sleep 5
            done
          fi

          # k8s/eks íŒ¨í„´ ë³´ì•ˆ ê·¸ë£¹ ì‚­ì œ
          for sg_id in $(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=k8s-*,eks-cluster-sg-*" \
            --query 'SecurityGroups[*].GroupId' --output text 2>/dev/null || true); do
            aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
          done

      - name: Post-Cleanup Complete
        run: echo "âœ… AWS Post-Cleanup ì™„ë£Œ"

  # ============================================
  # Slack ì•Œë¦¼ - ì™„ë£Œ
  # ============================================
  notify-complete:
    needs: [destroy, aws-post-cleanup]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Send Slack Notification - Complete
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "${{ needs.destroy.result == 'success' && 'âœ… Terraform Destroy ì„±ê³µ' || 'âŒ Terraform Destroy ì‹¤íŒ¨' }}",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Cloud:*\n${{ github.event.inputs.cloud }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Layer:*\n${{ github.event.inputs.layer }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*ê²°ê³¼:*\n${{ needs.destroy.result }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*ì‹¤í–‰ì:*\n${{ github.actor }}"
                    }
                  ]
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "ìƒì„¸ ë¡œê·¸ ë³´ê¸°",
                        "emoji": true
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

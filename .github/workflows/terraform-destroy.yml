# ============================================================================
# Terraform Destroy (Multi-Cloud: AWS / GCP)
# ============================================================================
# 수동 실행으로 클라우드와 레이어 선택하여 Destroy
# - AWS: Karpenter 정리, ALB 정리, Terraform destroy
# - GCP: GKE 정리, Terraform destroy
# ============================================================================

name: Terraform Destroy

on:
  workflow_dispatch:
    inputs:
      cloud:
        description: '클라우드 선택'
        required: true
        default: 'aws'
        type: choice
        options:
          - aws
          - gcp
      confirm:
        description: '삭제 확인 (destroy 입력)'
        required: true
        default: ''
      layer:
        description: '삭제 레이어 선택'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - bootstrap
          - compute
          - foundation

env:
  TF_VERSION: '1.9.0'
  TG_VERSION: '0.54.0'
  TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"

  # AWS 설정
  AWS_REGION: ap-northeast-2
  AWS_PROJECT_NAME: "petclinic-kr"

  # GCP 설정
  GCP_PROJECT_ID: kdt2-final-project-t1
  GCP_REGION: asia-northeast3
  GCP_CLUSTER_NAME: petclinic-dr-gke
  GCP_WORKLOAD_IDENTITY_PROVIDER: projects/605820610222/locations/global/workloadIdentityPools/github-pool/providers/github-provider

  # Slack
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # 확인 단계
  # ============================================================================
  confirm:
    name: 'Confirm Destroy'
    runs-on: ubuntu-latest
    steps:
      - name: Check confirmation
        if: github.event.inputs.confirm != 'destroy'
        run: |
          echo "❌ 삭제를 진행하려면 'destroy'를 입력하세요."
          exit 1

      - name: Confirmed
        run: |
          echo "✅ 삭제가 확인되었습니다."
          echo "Cloud: ${{ github.event.inputs.cloud }}"
          echo "Layer: ${{ github.event.inputs.layer }}"

  # ============================================================================
  # Slack 알림 - 승인 요청
  # ============================================================================
  notify-approval:
    name: Notify & Wait for Approval
    needs: [confirm]
    runs-on: ubuntu-latest
    steps:
      - name: Send Slack Notification - Approval Request
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "🚨 Terraform Destroy 승인 요청",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Cloud:*\n${{ github.event.inputs.cloud }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Layer:*\n${{ github.event.inputs.layer }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*실행자:*\n${{ github.actor }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*확인:*\n${{ github.event.inputs.confirm }}"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "🔴 *경고: 인프라 삭제가 요청되었습니다!* 승인 후 Destroy가 실행됩니다."
                    }
                  ]
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "승인하러 가기",
                        "emoji": true
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
                      "style": "danger"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  # ============================================================================
  # AWS Pre-Cleanup (승인 후, AWS 선택 시)
  # ============================================================================
  aws-pre-cleanup:
    name: 'AWS Pre-Cleanup'
    needs: [notify-approval]
    if: github.event.inputs.cloud == 'aws'
    runs-on: ubuntu-latest
    environment: production  # 👈 승인 없으면 여기서 대기
    timeout-minutes: 15
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Configure kubectl
        id: configure-kubectl
        continue-on-error: true
        run: |
          CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
            aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
            echo "✅ EKS 클러스터 연결: $CLUSTER_NAME"
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ EKS 클러스터 없음 - Pre-cleanup 스킵"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Cleanup Karpenter Resources
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 8
        run: |
          echo "🧹 Karpenter 정리 시작..."

          # ============================================================
          # 핵심: ArgoCD가 Karpenter를 복구하지 못하도록 먼저 비활성화!
          # 순서: ArgoCD Sync 비활성화 → Controller 중지 → NodePool 삭제 → EC2 종료
          # ============================================================

          # 0. ArgoCD Karpenter Application Auto-Sync 비활성화 (selfHeal 방지)
          echo "  0️⃣ ArgoCD Karpenter Application Auto-Sync 비활성화..."
          kubectl patch application karpenter -n argocd --type merge \
            -p '{"spec":{"syncPolicy":null}}' 2>/dev/null || true
          kubectl patch application karpenter-config -n argocd --type merge \
            -p '{"spec":{"syncPolicy":null}}' 2>/dev/null || true

          # 1. Karpenter Controller 중지 (가장 먼저! 새 노드 생성 방지)
          echo "  1️⃣ Karpenter Controller 중지..."
          kubectl scale deployment karpenter -n kube-system --replicas=0 --timeout=60s 2>/dev/null || true

          # Controller가 완전히 중지될 때까지 대기
          echo "    Controller 중지 대기 중..."
          for i in {1..6}; do
            RUNNING_PODS=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=karpenter --field-selector=status.phase=Running -o name 2>/dev/null | wc -l)
            if [ "$RUNNING_PODS" -eq 0 ]; then
              echo "    ✅ Controller 중지 완료"
              break
            fi
            echo "    대기 중... ($i/6)"
            sleep 5
          done

          # 2. NodePool 삭제 (새 노드 생성 방지 - Controller가 재시작해도 NodePool 없으면 생성 불가)
          echo "  2️⃣ NodePool 삭제..."
          kubectl get nodepools -o name 2>/dev/null | while read np; do
            kubectl patch $np -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $np --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 3. EC2NodeClass 삭제
          echo "  3️⃣ EC2NodeClass 삭제..."
          kubectl get ec2nodeclasses -o name 2>/dev/null | while read ec; do
            kubectl patch $ec -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $ec --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 4. NodeClaim Finalizer 제거 및 삭제
          echo "  4️⃣ NodeClaim 삭제..."
          kubectl get nodeclaims -o name 2>/dev/null | while read nc; do
            kubectl patch $nc -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $nc --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # 5. EC2 인스턴스 강제 종료 (Controller 중지 후 안전하게 종료)
          echo "  5️⃣ Karpenter EC2 인스턴스 강제 종료..."
          # tag-key 필터 사용 (Values=*는 와일드카드로 작동하지 않음)
          KARPENTER_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # 이름 패턴으로도 검색 (karpenter-node 패턴)
          KARPENTER_BY_NAME=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=*karpenter*" "Name=instance-state-name,Values=running,pending,stopping" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)

          # 두 결과 합치기 (중복 제거)
          ALL_KARPENTER_INSTANCES=$(echo "$KARPENTER_INSTANCES $KARPENTER_BY_NAME" | tr ' ' '\n' | sort -u | tr '\n' ' ' | xargs)

          if [ -n "$ALL_KARPENTER_INSTANCES" ]; then
            echo "    종료할 인스턴스: $ALL_KARPENTER_INSTANCES"
            aws ec2 terminate-instances --instance-ids $ALL_KARPENTER_INSTANCES 2>/dev/null || true
          else
            echo "    Karpenter EC2 인스턴스 없음"
          fi

          # 6. Karpenter 노드 삭제 (K8s에서 노드 객체 제거)
          echo "  6️⃣ Karpenter 노드 삭제..."
          KARPENTER_NODES=$(kubectl get nodes -l karpenter.sh/nodepool -o name 2>/dev/null || true)
          if [ -n "$KARPENTER_NODES" ]; then
            echo "$KARPENTER_NODES" | while read node; do
              echo "    삭제 중: $node"
              kubectl delete $node --force --grace-period=0 --timeout=60s 2>/dev/null || true
            done
          else
            echo "    Karpenter 노드 없음"
          fi

          # 7. EC2 인스턴스 종료 확인 (최대 2분 대기)
          echo "  7️⃣ EC2 인스턴스 종료 확인..."
          for i in {1..12}; do
            REMAINING=$(aws ec2 describe-instances \
              --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending,stopping,shutting-down" \
              --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)
            if [ -z "$REMAINING" ]; then
              echo "    ✅ 모든 Karpenter 인스턴스 종료 완료"
              break
            fi
            echo "    대기 중... ($i/12) - 남은 인스턴스: $REMAINING"
            sleep 10
          done

          # 8. 혹시 남은 인스턴스 한번 더 종료 시도
          FINAL_CHECK=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" "Name=instance-state-name,Values=running,pending" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)
          if [ -n "$FINAL_CHECK" ]; then
            echo "  ⚠️ 남은 인스턴스 재종료: $FINAL_CHECK"
            aws ec2 terminate-instances --instance-ids $FINAL_CHECK 2>/dev/null || true
          fi

          echo "✅ Karpenter 정리 완료"

      - name: Cleanup ArgoCD Applications
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "🧹 ArgoCD 정리 시작..."

          # Application Finalizer 제거 및 삭제
          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $app -n argocd --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          echo "✅ ArgoCD 정리 완료"

      - name: Cleanup Terraform-managed K8s Resources
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "🧹 Terraform 관리 K8s 리소스 정리 시작..."

          # ============================================================
          # 1. StorageClass 삭제 (Terraform에서 삭제 권한 에러 방지)
          # ============================================================
          echo "  1️⃣ StorageClass 삭제..."
          # gp3 StorageClass 삭제 (Terraform이 생성한 것)
          kubectl delete storageclass gp3 --timeout=30s 2>/dev/null || true

          # gp2 annotation 제거 시도
          kubectl annotate storageclass gp2 storageclass.kubernetes.io/is-default-class- 2>/dev/null || true

          # ============================================================
          # 2. Helm Releases 삭제 (Terraform에서 생성한 것들)
          # ============================================================
          echo "  2️⃣ Helm Releases 삭제..."

          # kube-prometheus-stack 삭제
          helm uninstall kube-prometheus-stack -n petclinic --timeout 3m 2>/dev/null || true

          # PVC 삭제 (Prometheus, Grafana, Alertmanager)
          kubectl delete pvc -n petclinic -l app.kubernetes.io/name=prometheus --timeout=60s 2>/dev/null || true
          kubectl delete pvc -n petclinic -l app.kubernetes.io/name=grafana --timeout=60s 2>/dev/null || true
          kubectl delete pvc -n petclinic -l app.kubernetes.io/name=alertmanager --timeout=60s 2>/dev/null || true

          # 남은 모든 PVC 삭제
          kubectl delete pvc -n petclinic --all --timeout=60s 2>/dev/null || true

          # ============================================================
          # 3. EKS Add-ons 삭제 (Terraform에서 생성한 것들)
          # ============================================================
          echo "  3️⃣ EKS Add-ons 삭제..."

          CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
            # aws-ebs-csi-driver 삭제
            echo "    EBS CSI Driver 삭제..."
            aws eks delete-addon --cluster-name $CLUSTER_NAME --addon-name aws-ebs-csi-driver 2>/dev/null || true

            # vpc-cni 삭제
            echo "    VPC CNI 삭제..."
            aws eks delete-addon --cluster-name $CLUSTER_NAME --addon-name vpc-cni 2>/dev/null || true

            # Add-ons 삭제 대기 (최대 2분)
            echo "    Add-ons 삭제 대기..."
            for i in {1..12}; do
              ADDON_COUNT=$(aws eks list-addons --cluster-name $CLUSTER_NAME --query 'length(addons)' --output text 2>/dev/null || echo "0")
              if [ "$ADDON_COUNT" = "0" ]; then
                echo "    ✅ Add-ons 삭제 완료"
                break
              fi
              ADDONS=$(aws eks list-addons --cluster-name $CLUSTER_NAME --query 'addons[*]' --output text 2>/dev/null || true)
              echo "      대기 중... ($i/12) - 남은 Add-ons: $ADDONS"
              sleep 10
            done
          fi

          # ============================================================
          # 4. petclinic namespace 삭제 (Helm에서 생성한 것)
          # ============================================================
          echo "  4️⃣ petclinic namespace 삭제..."
          kubectl delete namespace petclinic --timeout=60s 2>/dev/null || true

          echo "✅ Terraform 관리 K8s 리소스 정리 완료"

      - name: Cleanup Ingress and LoadBalancer Services
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 3
        run: |
          echo "🧹 Ingress 및 LoadBalancer 정리 시작..."

          # 모든 Ingress 삭제
          kubectl get ingress -A -o json 2>/dev/null | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl patch ingress $name -n $ns -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete ingress $name -n $ns --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # LoadBalancer 타입 Service 삭제
          kubectl get svc -A -o json 2>/dev/null | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl delete svc $name -n $ns --timeout=60s 2>/dev/null || true
          done

          echo "✅ Ingress 및 LoadBalancer 정리 완료"

      - name: Force Delete ALB and Target Groups
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "🧹 ALB 및 Target Group 강제 삭제..."

          # ============================================================
          # petclinic VPC의 모든 ALB 삭제 (VPC 기반 - 누락 방지)
          # ============================================================

          # petclinic VPC ID 찾기
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          # ============================================================
          # 1단계: VPC 내 모든 ALB 삭제
          # ============================================================
          echo "  1️⃣ VPC 내 모든 ALB 삭제..."
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "  VPC ID: $VPC_ID"

            # VPC 내 모든 ALB 조회
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text 2>/dev/null || true)
          else
            echo "  VPC 없음 - 패턴 기반 삭제로 fallback"
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'monitoring')].LoadBalancerArn" \
              --output text 2>/dev/null || true)
          fi

          if [ -z "$ALB_ARNS" ]; then
            echo "  삭제할 ALB 없음"
          else
            for alb_arn in $ALB_ARNS; do
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $alb_arn --query 'LoadBalancers[0].LoadBalancerName' --output text 2>/dev/null || echo "unknown")
              echo "  삭제 중: $ALB_NAME ($alb_arn)"

              # Listener 먼저 삭제
              for listener_arn in $(aws elbv2 describe-listeners --load-balancer-arn $alb_arn --query 'Listeners[*].ListenerArn' --output text 2>/dev/null); do
                echo "    Listener 삭제: $listener_arn"
                aws elbv2 delete-listener --listener-arn $listener_arn 2>/dev/null || true
              done

              # ALB 삭제
              aws elbv2 delete-load-balancer --load-balancer-arn $alb_arn 2>/dev/null || true
            done
          fi

          # ============================================================
          # 2단계: ALB 완전 삭제 대기 (최대 3분)
          # ============================================================
          echo "  2️⃣ ALB 완전 삭제 대기..."
          for i in {1..36}; do
            # VPC 기반 또는 패턴 기반으로 남은 ALB 확인
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              REMAINING_ALBS=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            else
              REMAINING_ALBS=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'monitoring')].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$REMAINING_ALBS" ]; then
              echo "  ✅ 모든 ALB 삭제 완료"
              break
            fi

            echo "    대기 중... ($i/36)"
            echo "$REMAINING_ALBS" | while read name state; do
              echo "      - $name: $state"
            done
            sleep 5
          done

          # ============================================================
          # 3단계: Target Group 삭제 (ALB 삭제 완료 후)
          # VPC 기반으로 모든 Target Group 삭제
          # ============================================================
          echo "  3️⃣ Target Group 삭제..."

          # Target Group 삭제 재시도 (최대 3회)
          for retry in {1..3}; do
            # VPC 기반 Target Group 조회
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              TG_ARNS=$(aws elbv2 describe-target-groups \
                --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
                --output text 2>/dev/null || true)
            else
              TG_ARNS=$(aws elbv2 describe-target-groups \
                --query "TargetGroups[?contains(TargetGroupName, 'k8s') || contains(TargetGroupName, 'petclinic') || contains(TargetGroupName, 'monitoring')].TargetGroupArn" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$TG_ARNS" ]; then
              echo "  ✅ 모든 Target Group 삭제 완료"
              break
            fi

            echo "    Target Group 삭제 시도 ($retry/3)..."
            for tg_arn in $TG_ARNS; do
              TG_NAME=$(aws elbv2 describe-target-groups --target-group-arns $tg_arn --query 'TargetGroups[0].TargetGroupName' --output text 2>/dev/null || echo "unknown")
              echo "      삭제 중: $TG_NAME"
              aws elbv2 delete-target-group --target-group-arn $tg_arn 2>/dev/null || true
            done

            # 삭제 대기
            sleep 10
          done

          echo "✅ ALB 및 Target Group 정리 완료"

      - name: Wait for ALB Deletion
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "⏳ AWS 로드밸런서 최종 삭제 확인..."

          # petclinic VPC ID 찾기
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          # ALB 최종 삭제 확인 (최대 3분 대기)
          for i in {1..18}; do
            # VPC 기반 ALB 확인
            if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
              ALB_LIST=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            else
              ALB_LIST=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?contains(LoadBalancerName, 'k8s') || contains(LoadBalancerName, 'argocd') || contains(LoadBalancerName, 'petclinic') || contains(LoadBalancerName, 'monitoring')].[LoadBalancerName,State.Code]" \
                --output text 2>/dev/null || true)
            fi

            if [ -z "$ALB_LIST" ]; then
              echo "✅ 모든 ALB 삭제 완료"
              break
            fi

            echo "  대기 중... ($i/18)"
            echo "$ALB_LIST" | while read name state; do
              echo "    - $name: $state"
              # deleting 상태가 아닌 ALB가 있으면 다시 삭제 시도
              if [ "$state" != "deleting" ]; then
                ALB_ARN=$(aws elbv2 describe-load-balancers --names "$name" --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || true)
                if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
                  echo "      재삭제 시도..."
                  aws elbv2 delete-load-balancer --load-balancer-arn "$ALB_ARN" 2>/dev/null || true
                fi
              fi
            done
            sleep 10
          done

          # Target Group 최종 확인 (VPC 기반)
          echo "⏳ Target Group 최종 확인..."
          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            TG_REMAINING=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupName" \
              --output text 2>/dev/null || true)
          else
            TG_REMAINING=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?contains(TargetGroupName, 'k8s') || contains(TargetGroupName, 'petclinic') || contains(TargetGroupName, 'monitoring')].TargetGroupName" \
              --output text 2>/dev/null || true)
          fi

          if [ -n "$TG_REMAINING" ]; then
            echo "  남은 Target Group 재삭제 시도..."
            for tg_name in $TG_REMAINING; do
              TG_ARN=$(aws elbv2 describe-target-groups --names "$tg_name" --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || true)
              if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
                aws elbv2 delete-target-group --target-group-arn "$TG_ARN" 2>/dev/null || true
              fi
            done
          fi

          echo "✅ ALB 최종 확인 완료"

      # ============================================================
      # EKS 클러스터 없어도 ALB 삭제 필요 (클러스터 삭제 후에도 ALB 남을 수 있음)
      # ============================================================
      - name: Force Delete Remaining ALB (No Cluster)
        if: steps.configure-kubectl.outputs.cluster_exists != 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "🧹 클러스터 없이 남은 ALB 삭제..."

          # petclinic VPC ID 찾기
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
            echo "  petclinic VPC 없음 - 스킵"
          else
            echo "  VPC ID: $VPC_ID"

            # VPC 내 모든 ALB 삭제
            ALB_ARNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
              --output text 2>/dev/null || true)

            for alb_arn in $ALB_ARNS; do
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $alb_arn --query 'LoadBalancers[0].LoadBalancerName' --output text 2>/dev/null || echo "unknown")
              echo "  삭제 중: $ALB_NAME"

              # Listener 삭제
              for listener_arn in $(aws elbv2 describe-listeners --load-balancer-arn $alb_arn --query 'Listeners[*].ListenerArn' --output text 2>/dev/null); do
                aws elbv2 delete-listener --listener-arn $listener_arn 2>/dev/null || true
              done

              # ALB 삭제
              aws elbv2 delete-load-balancer --load-balancer-arn $alb_arn 2>/dev/null || true
            done

            # ALB 삭제 대기
            echo "  ALB 삭제 대기..."
            for i in {1..18}; do
              REMAINING=$(aws elbv2 describe-load-balancers \
                --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerName" \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  ✅ 모든 ALB 삭제 완료"
                break
              fi
              echo "    대기 중... ($i/18) - 남은 ALB: $REMAINING"
              sleep 10
            done

            # Target Group 삭제
            echo "  Target Group 삭제..."
            TG_ARNS=$(aws elbv2 describe-target-groups \
              --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" \
              --output text 2>/dev/null || true)
            for tg_arn in $TG_ARNS; do
              aws elbv2 delete-target-group --target-group-arn $tg_arn 2>/dev/null || true
            done
          fi

          echo "✅ 남은 ALB 삭제 완료"

      - name: Pre-Cleanup Complete
        run: echo "✅ AWS Pre-Cleanup 완료"

  # ============================================================================
  # GCP Pre-Cleanup (승인 후, GCP 선택 시)
  # ============================================================================
  gcp-pre-cleanup:
    name: 'GCP Pre-Cleanup'
    needs: [notify-approval]
    if: github.event.inputs.cloud == 'gcp'
    runs-on: ubuntu-latest
    environment: production  # 👈 승인 없으면 여기서 대기
    timeout-minutes: 15
    steps:
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: github-actions@${{ env.GCP_PROJECT_ID }}.iam.gserviceaccount.com

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Configure kubectl
        id: configure-kubectl
        continue-on-error: true
        run: |
          if gcloud container clusters describe ${{ env.GCP_CLUSTER_NAME }} --region ${{ env.GCP_REGION }} 2>/dev/null; then
            gcloud container clusters get-credentials ${{ env.GCP_CLUSTER_NAME }} \
              --region ${{ env.GCP_REGION }} \
              --project ${{ env.GCP_PROJECT_ID }}
            echo "✅ GKE 클러스터 연결: ${{ env.GCP_CLUSTER_NAME }}"
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ GKE 클러스터 없음 - Pre-cleanup 스킵"
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Cleanup ArgoCD Applications
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "🧹 ArgoCD 정리 시작..."

          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete $app -n argocd --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          echo "✅ ArgoCD 정리 완료"

      - name: Cleanup Ingress and LoadBalancer Services
        if: steps.configure-kubectl.outputs.cluster_exists == 'true'
        continue-on-error: true
        timeout-minutes: 2
        run: |
          echo "🧹 Ingress 및 LoadBalancer 정리 시작..."

          # 모든 Ingress 삭제
          kubectl get ingress -A -o json 2>/dev/null | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl patch ingress $name -n $ns -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
            kubectl delete ingress $name -n $ns --force --grace-period=0 --timeout=30s 2>/dev/null || true
          done

          # LoadBalancer 타입 Service 삭제
          kubectl get svc -A -o json 2>/dev/null | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read ns name; do
            [ -z "$ns" ] && continue
            kubectl delete svc $name -n $ns --timeout=60s 2>/dev/null || true
          done

          echo "✅ Ingress 및 LoadBalancer 정리 완료"

          # GKE Ingress Controller가 LB 리소스를 정리할 시간 대기
          echo "⏳ GKE Ingress Controller가 LB 리소스 정리 중... (60초 대기)"
          sleep 60

      - name: Cleanup GKE Ingress Resources (LB Stack → Backend → NEG)
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "🧹 GKE Ingress 리소스 정리 시작 (병렬 처리, 최대 10분)..."
          echo "   삭제 순서: Forwarding Rules → Target Proxies → URL Maps → Backend Services → Health Checks → NEG → Firewall"
          echo "   완료 시 즉시 다음 단계로 진행"

          # ============================================================
          # 1-5단계: LB 리소스 삭제 (병렬)
          # ============================================================
          echo "  1️⃣ Forwarding Rules 삭제 (병렬)..."
          gcloud compute forwarding-rules list --filter="name~k8s OR name~tf-" --format="value(name)" --global 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute forwarding-rules delete "{}" --global --quiet 2>/dev/null || true

          echo "  2️⃣ Target Proxies 삭제 (병렬)..."
          gcloud compute target-http-proxies list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute target-http-proxies delete "{}" --global --quiet 2>/dev/null || true
          gcloud compute target-https-proxies list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute target-https-proxies delete "{}" --global --quiet 2>/dev/null || true

          echo "  3️⃣ URL Maps 삭제 (병렬)..."
          gcloud compute url-maps list --filter="name~k8s OR name~tf-" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute url-maps delete "{}" --global --quiet 2>/dev/null || true

          echo "  4️⃣ Backend Services 삭제 (병렬)..."
          gcloud compute backend-services list --filter="name~k8s OR name~tf-" --format="value(name)" --global 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute backend-services delete "{}" --global --quiet 2>/dev/null || true

          echo "  5️⃣ Health Checks 삭제 (병렬)..."
          gcloud compute health-checks list --filter="name~k8s" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute health-checks delete "{}" --global --quiet 2>/dev/null || true

          # ============================================================
          # 6단계: NEG 삭제 (완료 기반 대기, 최대 5분)
          # ============================================================
          echo "  6️⃣ NEG 삭제 (병렬, 완료 시 즉시 진행)..."

          for attempt in {1..30}; do  # 30 x 10s = 5분 최대
            # 현재 NEG 목록 확인
            NEG_COUNT=$(gcloud compute network-endpoint-groups list \
              --filter="name~k8s1 OR name~k8s2 OR name~petclinic" \
              --format="value(name)" 2>/dev/null | wc -l)

            if [ "$NEG_COUNT" -eq 0 ]; then
              echo "    ✅ 모든 NEG 삭제 완료!"
              break
            fi

            echo "    남은 NEG: $NEG_COUNT개 (시도 $attempt/30)"

            # zone별 병렬 삭제 시도
            for zone in asia-northeast3-a asia-northeast3-b asia-northeast3-c; do
              (
                gcloud compute network-endpoint-groups list \
                  --filter="zone:$zone AND (name~k8s1 OR name~k8s2 OR name~petclinic)" \
                  --format="value(name)" 2>/dev/null | \
                  xargs -P 10 -I {} gcloud compute network-endpoint-groups delete "{}" --zone="$zone" --quiet 2>/dev/null || true
              ) &
            done
            wait

            sleep 10
          done

          # ============================================================
          # 7단계: 방화벽 삭제 (병렬)
          # ============================================================
          echo "  7️⃣ 방화벽 규칙 삭제 (병렬)..."
          gcloud compute firewall-rules list --filter="name~k8s" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute firewall-rules delete "{}" --quiet 2>/dev/null || true

          echo "✅ GKE Ingress 리소스 정리 완료"

      - name: Delete Cloud SQL Instance (Must be deleted before VPC Peering)
        continue-on-error: true
        timeout-minutes: 10
        run: |
          echo "🧹 Cloud SQL 인스턴스 삭제 (VPC Peering 삭제 전 필수)..."

          # Cloud SQL 인스턴스 목록 조회 및 삭제
          for instance in $(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null); do
            echo "  Cloud SQL 삭제: $instance"
            # deletion protection 해제
            gcloud sql instances patch "$instance" --no-deletion-protection --quiet 2>/dev/null || true
            # 인스턴스 삭제
            gcloud sql instances delete "$instance" --quiet 2>/dev/null || true
          done

          # Cloud SQL 삭제 대기 (최대 8분 - Cloud SQL 삭제는 시간이 오래 걸림)
          echo "  Cloud SQL 삭제 대기..."
          for i in {1..48}; do
            REMAINING=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
            if [ "$REMAINING" -eq 0 ]; then
              echo "  ✅ Cloud SQL 삭제 완료"
              break
            fi
            echo "    대기 중... ($i/48) - 남은 인스턴스: $REMAINING"
            sleep 10
          done

          # 최종 확인
          FINAL_CHECK=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
          if [ "$FINAL_CHECK" -gt 0 ]; then
            echo "  ⚠️ Cloud SQL 인스턴스가 아직 삭제 중입니다. Terraform에서 처리합니다."
          fi

          echo "✅ Cloud SQL 정리 완료"

      - name: Final Cleanup - NEG and Firewall (Before VPC deletion)
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "🧹 VPC 삭제 전 NEG 및 방화벽 규칙 최종 정리 (완료 시 즉시 진행)..."

          # ============================================================
          # NEG 최종 정리 (완료 기반 대기, 최대 3분)
          # ============================================================
          echo "  1️⃣ 남은 NEG 최종 삭제..."

          for attempt in {1..18}; do  # 18 x 10s = 3분 최대
            # 현재 NEG 목록 확인
            NEG_COUNT=$(gcloud compute network-endpoint-groups list \
              --filter="name~k8s1 OR name~k8s2 OR name~petclinic" \
              --format="value(name)" 2>/dev/null | wc -l)

            if [ "$NEG_COUNT" -eq 0 ]; then
              echo "    ✅ 모든 NEG 삭제 완료!"
              break
            fi

            echo "    남은 NEG: $NEG_COUNT개 (시도 $attempt/18)"

            # zone별 병렬 삭제 시도
            for zone in asia-northeast3-a asia-northeast3-b asia-northeast3-c; do
              (
                gcloud compute network-endpoint-groups list \
                  --filter="zone:$zone AND (name~k8s1 OR name~k8s2 OR name~petclinic)" \
                  --format="value(name)" 2>/dev/null | \
                  xargs -P 10 -I {} gcloud compute network-endpoint-groups delete "{}" --zone="$zone" --quiet 2>/dev/null || true
              ) &
            done
            wait

            sleep 10
          done

          # 최종 상태 확인
          REMAINING=$(gcloud compute network-endpoint-groups list --filter="name~k8s1 OR name~k8s2 OR name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
          if [ "$REMAINING" -gt 0 ]; then
            echo "  ⚠️ 남은 NEG: $REMAINING개 (Terraform에서 처리)"
          fi

          # ============================================================
          # 방화벽 규칙 최종 정리 (병렬)
          # ============================================================
          echo "  2️⃣ 방화벽 규칙 최종 삭제 (병렬)..."

          # VPC 관련 방화벽 + k8s 패턴 방화벽 병렬 삭제
          gcloud compute firewall-rules list --filter="name~k8s OR network~petclinic" --format="value(name)" 2>/dev/null | \
            xargs -P 10 -I {} gcloud compute firewall-rules delete "{}" --quiet 2>/dev/null || true

          echo "✅ NEG 및 방화벽 규칙 최종 정리 완료"

      - name: Cleanup VPC Resources (Peering, Routes, Addresses)
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "🧹 VPC 리소스 정리 시작..."

          # ============================================================
          # Cloud SQL 삭제 완료 확인 (Service Networking Connection 삭제 전 필수!)
          # ============================================================
          echo "  0️⃣ Cloud SQL 삭제 완료 확인..."
          for i in {1..12}; do
            SQL_COUNT=$(gcloud sql instances list --filter="name~petclinic" --format="value(name)" 2>/dev/null | wc -l)
            if [ "$SQL_COUNT" -eq 0 ]; then
              echo "  ✅ Cloud SQL 삭제 확인됨"
              break
            fi
            echo "    Cloud SQL 아직 삭제 중... ($i/12)"
            sleep 10
          done

          # petclinic 관련 VPC 목록 조회
          VPCS=$(gcloud compute networks list --filter="name~petclinic" --format="value(name)" 2>/dev/null || true)

          for vpc in $VPCS; do
            echo "  VPC: $vpc"

            # 1. Service Networking Connection 삭제 (Cloud SQL Private Service Connection)
            echo "    Service Networking Connection 삭제..."
            # Cloud SQL이 아직 삭제 중이면 실패할 수 있으므로 재시도
            for retry in {1..3}; do
              gcloud services vpc-peerings delete \
                --service=servicenetworking.googleapis.com \
                --network=$vpc \
                --quiet 2>/dev/null && break
              echo "    Service Networking 삭제 재시도... ($retry/3)"
              sleep 10
            done

            # 2. VPC Peering 삭제
            PEERINGS=$(gcloud compute networks peerings list --network=$vpc --format="value(name)" 2>/dev/null || true)
            for peering in $PEERINGS; do
              echo "    Peering 삭제: $peering"
              gcloud compute networks peerings delete $peering --network=$vpc --quiet 2>/dev/null || true
            done

            # 3. VPC에 연결된 Route 삭제
            ROUTES=$(gcloud compute routes list --filter="network:$vpc" --format="value(name)" 2>/dev/null || true)
            for route in $ROUTES; do
              echo "    Route 삭제: $route"
              gcloud compute routes delete $route --quiet 2>/dev/null || true
            done
          done

          # 4. Global Address 삭제 (Cloud SQL Private IP 등)
          echo "  Global Address 삭제..."
          for addr in $(gcloud compute addresses list --filter="name~petclinic" --format="value(name)" --global 2>/dev/null); do
            echo "    삭제: $addr"
            gcloud compute addresses delete "$addr" --global --quiet 2>/dev/null || true
          done

          echo "✅ VPC 리소스 정리 완료"

      - name: Pre-Cleanup Complete
        run: echo "✅ GCP Pre-Cleanup 완료"

  # ============================================================================
  # Terraform Destroy (승인 및 Pre-Cleanup 후 실행)
  # ============================================================================
  destroy:
    name: 'Terraform Destroy'
    needs: [notify-approval, aws-pre-cleanup, gcp-pre-cleanup]
    if: always() && needs.notify-approval.result == 'success'
    runs-on: ubuntu-latest
    timeout-minutes: 60

    defaults:
      run:
        working-directory: ${{ github.event.inputs.cloud }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # AWS 인증
      - name: Configure AWS credentials
        if: github.event.inputs.cloud == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # GCP 인증
      - name: Authenticate to Google Cloud
        if: github.event.inputs.cloud == 'gcp'
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ env.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: github-actions@${{ env.GCP_PROJECT_ID }}.iam.gserviceaccount.com

      - name: Set up Cloud SDK
        if: github.event.inputs.cloud == 'gcp'
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ env.GCP_PROJECT_ID }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Setup Terragrunt
        run: |
          wget -q https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64
          chmod +x terragrunt_linux_amd64
          sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt

      # SSH Key (AWS 전용)
      - name: Create SSH Key
        if: github.event.inputs.cloud == 'aws'
        run: |
          mkdir -p keys
          echo "${{ secrets.SSH_PUBLIC_KEY }}" > keys/test.pub

      # ArgoCD Finalizer 제거 (Namespace 삭제 타임아웃 방지)
      - name: Remove ArgoCD Finalizers
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        continue-on-error: true
        run: |
          echo "🔧 ArgoCD Finalizer 제거 중..."
          # ArgoCD Application finalizer 제거
          kubectl get applications -n argocd -o name 2>/dev/null | while read app; do
            kubectl patch $app -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge 2>/dev/null || true
          done
          # ArgoCD namespace finalizer 제거
          kubectl get namespace argocd -o json 2>/dev/null | \
            jq '.spec.finalizers = []' | \
            kubectl replace --raw "/api/v1/namespaces/argocd/finalize" -f - 2>/dev/null || true
          echo "✅ Finalizer 제거 완료"

      # Destroy Bootstrap (먼저! ArgoCD, Helm 릴리스 정리)
      - name: Destroy Bootstrap
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        continue-on-error: true
        timeout-minutes: 15
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "🗑️ Bootstrap Layer Destroy (ArgoCD, Helm 먼저 삭제)..."
          cd bootstrap && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      # Destroy Compute (Bootstrap 삭제 후! GKE/EKS, DB 삭제)
      - name: Destroy Compute
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'compute'
        continue-on-error: true
        timeout-minutes: 30
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "🗑️ Compute Layer Destroy (GKE/EKS, DB 삭제)..."
          cd compute && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      # VPC 삭제 전 보안그룹 강제 삭제 (AWS only)
      - name: Force Delete Security Groups Before VPC
        if: github.event.inputs.cloud == 'aws' && (github.event.inputs.layer == 'all' || github.event.inputs.layer == 'foundation')
        continue-on-error: true
        timeout-minutes: 5
        run: |
          echo "🧹 VPC 삭제 전 보안 그룹 강제 삭제..."

          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*petclinic*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || true)

          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            echo "  VPC ID: $VPC_ID"

            SG_LIST=$(aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'SecurityGroups[*].GroupId' --output text 2>/dev/null || true)

            # 1단계: 모든 규칙 제거
            echo "  1️⃣ 보안 그룹 규칙 제거..."
            for sg_id in $SG_LIST; do
              SG_NAME=$(aws ec2 describe-security-groups --group-ids $sg_id \
                --query 'SecurityGroups[0].GroupName' --output text 2>/dev/null || echo "unknown")
              [ "$SG_NAME" = "default" ] && continue

              # Ingress 규칙 제거
              for rule_id in $(aws ec2 describe-security-group-rules \
                --filters "Name=group-id,Values=$sg_id" \
                --query 'SecurityGroupRules[?IsEgress==`false`].SecurityGroupRuleId' \
                --output text 2>/dev/null || true); do
                [ -z "$rule_id" ] || [ "$rule_id" = "None" ] && continue
                aws ec2 revoke-security-group-ingress --group-id $sg_id \
                  --security-group-rule-ids $rule_id 2>/dev/null || true
              done

              # Egress 규칙 제거
              for rule_id in $(aws ec2 describe-security-group-rules \
                --filters "Name=group-id,Values=$sg_id" \
                --query 'SecurityGroupRules[?IsEgress==`true`].SecurityGroupRuleId' \
                --output text 2>/dev/null || true); do
                [ -z "$rule_id" ] || [ "$rule_id" = "None" ] && continue
                aws ec2 revoke-security-group-egress --group-id $sg_id \
                  --security-group-rule-ids $rule_id 2>/dev/null || true
              done
            done

            # 2단계: 보안 그룹 삭제
            echo "  2️⃣ 보안 그룹 삭제..."
            for sg_id in $SG_LIST; do
              SG_NAME=$(aws ec2 describe-security-groups --group-ids $sg_id \
                --query 'SecurityGroups[0].GroupName' --output text 2>/dev/null || echo "unknown")
              [ "$SG_NAME" = "default" ] && continue
              echo "    삭제: $sg_id ($SG_NAME)"
              aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
            done

            # 3단계: 재시도 (의존성으로 실패한 경우)
            echo "  3️⃣ 남은 보안 그룹 재시도..."
            for i in {1..3}; do
              REMAINING=$(aws ec2 describe-security-groups \
                --filters "Name=vpc-id,Values=$VPC_ID" \
                --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
                --output text 2>/dev/null || true)
              if [ -z "$REMAINING" ]; then
                echo "  ✅ 모든 보안 그룹 삭제 완료"
                break
              fi
              for sg_id in $REMAINING; do
                aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
              done
              sleep 5
            done
          else
            echo "  petclinic VPC 없음 - 스킵"
          fi

          # k8s/eks 패턴 보안 그룹 삭제 (VPC 외부)
          echo "  4️⃣ k8s/eks 패턴 보안 그룹 삭제..."
          for sg_id in $(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=k8s-*,eks-cluster-sg-*" \
            --query 'SecurityGroups[*].GroupId' --output text 2>/dev/null || true); do
            echo "    삭제: $sg_id"
            aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || true
          done

          echo "✅ 보안 그룹 강제 삭제 완료"

      # Destroy Foundation
      - name: Destroy Foundation
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'foundation'
        continue-on-error: true
        timeout-minutes: 15
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "🗑️ Foundation Layer Destroy..."
          cd foundation && terragrunt destroy --terragrunt-non-interactive -auto-approve || true

      - name: Destroy Complete
        run: |
          echo "=============================================="
          echo "🎉 Terraform Destroy 완료!"
          echo "=============================================="
          echo "Cloud: ${{ github.event.inputs.cloud }}"
          echo "Layer: ${{ github.event.inputs.layer }}"

  # ============================================
  # Slack 알림 - 완료
  # ============================================
  notify-complete:
    needs: [destroy]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Send Slack Notification - Complete
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "${{ needs.destroy.result == 'success' && '✅ Terraform Destroy 성공' || '❌ Terraform Destroy 실패' }}",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Cloud:*\n${{ github.event.inputs.cloud }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Layer:*\n${{ github.event.inputs.layer }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*결과:*\n${{ needs.destroy.result }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*실행자:*\n${{ github.actor }}"
                    }
                  ]
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "상세 로그 보기",
                        "emoji": true
                      },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  ]
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
